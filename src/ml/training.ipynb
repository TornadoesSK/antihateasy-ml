{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset, DataLoader, SubsetRandomSampler\n",
    "from pytorch_lightning import Trainer, LightningModule, LightningDataModule, seed_everything, Callback\n",
    "from torchmetrics import Accuracy, Precision, Recall, Specificity, AUROC\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "GLOBAL_SEED = 0\n",
    "seed_everything(GLOBAL_SEED, workers=True)\n",
    "CUDA_DEVICE_COUNT = torch.cuda.device_count()\n",
    "DATA_PATH = \"../../../data-davidson/data/labeled_data.csv\"\n",
    "NUM_WORKERS = 0\n",
    "MAX_LENGTH = 50\n",
    "DIMS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.glove = GloVe(name=\"6B\", dim=DIMS)\n",
    "\n",
    "    def collate_preprocess_data(self, batch):\n",
    "        x_arr, y_arr = [], []\n",
    "        for tweet, label in batch:\n",
    "            tokenized = self.tokenizer(tweet)\n",
    "            tweet = self.glove.get_vecs_by_tokens(tokenized)[:MAX_LENGTH, :]\n",
    "            padding = torch.zeros(MAX_LENGTH-tweet.shape[0], DIMS)\n",
    "            tweet = torch.cat((tweet, padding), dim=0)\n",
    "            x_arr.append(tweet.unsqueeze(0))\n",
    "            y_arr.append(label)\n",
    "\n",
    "        x = torch.cat(x_arr)\n",
    "        y = torch.tensor(y_arr)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def inference_preprocess_data(self, x):\n",
    "        space_pattern = '\\s+'\n",
    "        giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        mention_regex = '@[\\w\\-]+'\n",
    "        x = re.sub(space_pattern, ' ', x)\n",
    "        x = re.sub(giant_url_regex, 'URLHERE', x)\n",
    "        x = re.sub(mention_regex, 'MENTIONHERE', x)\n",
    "\n",
    "        tokenized = self.tokenizer(x)\n",
    "        tweet = self.glove.get_vecs_by_tokens(tokenized)[:MAX_LENGTH, :]\n",
    "        padding = torch.zeros(MAX_LENGTH-tweet.shape[0], DIMS)\n",
    "        tweet = torch.cat((tweet, padding), dim=0)\n",
    "        tweet = tweet.unsqueeze(0)\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, path, fold_index = None, num_folds = None, batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.val_sampler = None\n",
    "        self.train_sampler = None\n",
    "        self.val_fold = None\n",
    "        self.train_fold = None\n",
    "        self.splits = None\n",
    "        self.val_dataset = None\n",
    "        self.train_dataset = None\n",
    "\n",
    "        self.fold_index = fold_index\n",
    "        self.num_folds = num_folds\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # self.preprocessor = preprocessor\n",
    "\n",
    "        \n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.glove = GloVe(name=\"6B\", dim=DIMS)\n",
    "\n",
    "\n",
    "    def read_csv_to_numpy(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "        data = data.drop([\"Unnamed: 0\", \"count\", \"hate_speech\", \"offensive_language\", \"neither\"], axis=1)\n",
    "        data = data.reindex(columns=[\"tweet\", \"class\"])\n",
    "\n",
    "        df = data.drop(\n",
    "            data[data[\"class\"] == 1]\n",
    "            .sample(frac=0.92)\n",
    "            .index\n",
    "        )\n",
    "\n",
    "        df = df.drop(\n",
    "            df[df[\"class\"] == 2]\n",
    "            .sample(frac=0.65)\n",
    "            .index\n",
    "        )\n",
    "\n",
    "        return df.to_numpy()\n",
    "\n",
    "    def preprocess_data(self, batch):\n",
    "        x_arr, y_arr = [], []\n",
    "        for tweet, label in batch:\n",
    "\n",
    "            space_pattern = '\\s+'\n",
    "            giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "                '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "            mention_regex = '@[\\w\\-]+'\n",
    "            tweet = re.sub(space_pattern, ' ', tweet)\n",
    "            tweet = re.sub(giant_url_regex, 'URLHERE', tweet)\n",
    "            tweet = re.sub(mention_regex, 'MENTIONHERE', tweet)\n",
    "\n",
    "\n",
    "\n",
    "            tokenized = self.tokenizer(tweet)\n",
    "            tweet = self.glove.get_vecs_by_tokens(tokenized)[:MAX_LENGTH, :]\n",
    "            padding = torch.zeros(MAX_LENGTH-tweet.shape[0], DIMS)\n",
    "            tweet = torch.cat((tweet, padding), dim=0)\n",
    "            x_arr.append(tweet.unsqueeze(0))\n",
    "            y_arr.append(label)\n",
    "\n",
    "        x = torch.cat(x_arr)\n",
    "        y = torch.tensor(y_arr)\n",
    "        \n",
    "        return x, y \n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        # self.train_dataset = self.read_csv_to_numpy(self.path)\n",
    "        # self.val_dataset = self.read_csv_to_numpy(self.path)\n",
    "        self.dataset = self.read_csv_to_numpy(self.path)\n",
    "        x_indices = list(range(len(self.dataset)))\n",
    "\n",
    "        # if self.fold_index is not None and self.num_folds is not None:\n",
    "        #     self.splits = list(\n",
    "        #         StratifiedKFold(self.num_folds, shuffle=True, random_state=GLOBAL_SEED)\n",
    "        #             .split(X=x_indices, y=self.train_dataset.targets)\n",
    "        #     )\n",
    "        #     train_i, val_i = self.splits[self.fold_index]\n",
    "        # else:\n",
    "        #     train_i, val_i = train_test_split(x_indices, test_size=0.2, stratify=self.train_dataset.targets, random_state=GLOBAL_SEED)\n",
    "        \n",
    "        train_i, val_i = train_test_split(x_indices, test_size=0.2, stratify=self.dataset[:, 1], random_state=GLOBAL_SEED)\n",
    "\n",
    "        self.train_sampler = SubsetRandomSampler(train_i)\n",
    "        self.val_sampler = SubsetRandomSampler(val_i)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, sampler=self.train_sampler, num_workers=NUM_WORKERS, collate_fn=self.preprocess_data)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, sampler=self.val_sampler, num_workers=NUM_WORKERS, collate_fn=self.preprocess_data)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(LightningModule):\n",
    "    def __init__(self, preprocessor, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # self.argmax = nn.Arg\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        self.accuracy = Accuracy(num_classes=3)\n",
    "        self.precision_metric = Precision()\n",
    "        self.recall = Recall()\n",
    "        self.rocauc = AUROC(pos_label=1)\n",
    "        self.specificity = Specificity()\n",
    "\n",
    "        # self.backbone = backbone\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv = nn.Conv2d(1, 32, 3)\n",
    "        self.fc1 = nn.Linear(DIMS*MAX_LENGTH, 128)\n",
    "        # self.fc1 = nn.Linear(32*48*48, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 3)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.relu(self.conv(x))\n",
    "        # x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # x = self.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # x = x.unsqueeze(1)\n",
    "        # y = y.unsqueeze(1).float()\n",
    "        logits = self(x)\n",
    "        \n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        preds = self.softmax(logits)\n",
    "        acc = self.accuracy(preds, y)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # self.log(\"losses\", {\"train_loss\": loss}, on_step=False, on_epoch=True)\n",
    "        # self.log(\"accuracies\", {\"train_acc\": acc}, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # x = x.unsqueeze(1)\n",
    "        # y = y.unsqueeze(1).float()\n",
    "        logits = self(x)\n",
    "\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        preds = self.softmax(logits)\n",
    "        acc = self.accuracy(preds, y)\n",
    "        pre = self.precision_metric(preds, y)\n",
    "        rec = self.recall(preds, y)\n",
    "        # spe = self.specificity(preds, y_int)\n",
    "        # rocauc = self.rocauc(preds, y_int)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.log('val_pre', pre, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_rec', rec, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.log('val_spe', spe, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        # self.log('val_rocauc', rocauc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "\n",
    "        # self.log(\"losses\", {\"val_loss\": loss}, on_step=False, on_epoch=True)\n",
    "        # self.log(\"accuracies\", {\"val_acc\": acc}, on_step=False, on_epoch=True)\n",
    "\n",
    "    def predict_sentence(self, sentence):\n",
    "        transformed = self.preprocessor.inference_preprocess_data(sentence)\n",
    "        transformed = torch.flatten(transformed, start_dim=1)\n",
    "        # [1, 50, 50]\n",
    "        logits = self(transformed).detach()\n",
    "        return (self.softmax(logits), torch.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/Users/danielforrai/opt/miniconda3/envs/bachelor-env/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name             | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0  | softmax          | Softmax          | 0     \n",
      "1  | loss             | CrossEntropyLoss | 0     \n",
      "2  | accuracy         | Accuracy         | 0     \n",
      "3  | precision_metric | Precision        | 0     \n",
      "4  | recall           | Recall           | 0     \n",
      "5  | rocauc           | AUROC            | 0     \n",
      "6  | specificity      | Specificity      | 0     \n",
      "7  | relu             | ReLU             | 0     \n",
      "8  | flatten          | Flatten          | 0     \n",
      "9  | conv             | Conv2d           | 320   \n",
      "10 | fc1              | Linear           | 320 K \n",
      "11 | fc2              | Linear           | 65.7 K\n",
      "12 | fc3              | Linear           | 387   \n",
      "-------------------------------------------------------\n",
      "386 K     Trainable params\n",
      "0         Non-trainable params\n",
      "386 K     Total params\n",
      "1.546     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielforrai/opt/miniconda3/envs/bachelor-env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/danielforrai/opt/miniconda3/envs/bachelor-env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/danielforrai/opt/miniconda3/envs/bachelor-env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 35/35 [00:27<00:00,  1.27it/s, loss=0.0165, v_num=28, val_loss=2.040, val_acc=0.557, val_pre=0.557, val_rec=0.557, train_loss=0.0158, train_acc=0.998]\n",
      "CPU times: user 28.4 s, sys: 2.78 s, total: 31.2 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed_everything(GLOBAL_SEED, workers=True)\n",
    "\n",
    "pp = Preprocess()\n",
    "model = Net(pp, learning_rate=1e-3)\n",
    "data = DataModule(path=DATA_PATH, batch_size=128)\n",
    "# data.setup()\n",
    "\n",
    "# d = next(iter(data.train_dataloader()))[0].unsqueeze(1)\n",
    "# model(d).shape\n",
    "trainer = Trainer(max_epochs=30, log_every_n_steps=20, gpus=CUDA_DEVICE_COUNT, deterministic=True)\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "#CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "trainer.fit(model=model, datamodule=data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5bc1c1bd0718088abbfd128e494961034fa09678758e3ccadbb62ed12465b326"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('bachelor-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
